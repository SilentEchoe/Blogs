---
title: vLLM 架构与分布式推理机制分析
date: 2026-01-31 10:542:00
tags: [人工智能,Agent]
category: AI
---

大规模语言模型（LLM）的在线推理需要同时满足**高吞吐、低延迟**，并实现**内存/显存的高效利用**等关键目标。这些需求进一步推动了云原生在AI基础设施中的落地，一系列围绕着 Kubernetes 原生构建的高性能分布式推理框架应运而生，例如 [**vLLM**](chatgpt://generic-entity?number=0)、[**TensorRT-LLM**](chatgpt://generic-entity?number=1)、[**FastServe**](chatgpt://generic-entity?number=2)、[**LightLLM**](chatgpt://generic-entity?number=3) 等。这些框架通过更先进的**显存管理**与**请求调度**策略，在 Kubernetes 环境中实现接近硬件上限的推理效率，并进一步提供**GPU 资源调度优化**、**多副本负载均衡**、**模型热加载**与**弹性伸缩**等能力，逐步成为现代模型推理服务的基础设施底座。



vLLM 源于 UC Berkeley的开源项目，它本身是一个专为高吞吐，低延迟LLM推理设计的系统。它的创新点在于分页的KV缓存管理 以及 连续批处理(Continuous Batching) 机制，通过这两个技术在高并发的场景下优于传统架构。
vLLM 主要围绕 **LLMEngine**展开，这是处理推理请求的核心类，它的内部包含模型执行，调度和缓存管理等子组件。vLLM V1 引擎由**引擎核心 (Engine Core)** 和**服务层 (Serving layer)**构成。引擎核心负责单机多GPU的高效推理，服务层则处理多实例分布式部署和异步 Web 服务封装。



Scheduler 集中调度器：这个调度器位于引擎核心位置，负责决策每一步迭代中哪些请求参与计算。同时因vLLM 调度器实现了连续批处理策略，每轮迭代动态加入新请求或等待的请求，这使GPU时刻满载。调度器内部维护两个队列：waiting 和 running。支持先到先服务或优先级调度模式。它还集成了 KV缓存管理器，负责分配和释放 KV缓存的内存块，是分页注意力的核心。



KV 缓存管理：vLLM采用 **PagedAttention**算法，将注意力的 KV 缓存拆分为固定大小的"页"(内存块)管理。每个请求不再预先占用一整块连续显存，而是在需要时动态分配页面。一个轻量级的页表维护逻辑序列到无力内存块的映射，实现按需加载和服用。多个请求若有相同前缀，可共享相应的 KV 缓存页（写时复制机制），极大提高内存利用率。

这很像Linux中的分页，这种按序列整页交换避免了碎片和频繁拷贝的开销，通过实测 PagedAttention 可将 KV 缓存利用率提升到 90%以上，而传统连续内存分配往往只有 20~40% 利用率。如果显存不足，vLLM甚至会将整个请求的 KV 数据分页换出到 CPU 内存，在需要时换入。

上述分页存储和共享机制几乎**消除了内存碎片**，允许模型处理更长的上下文而不浪费显存。相较于传统一次性为每个请求分配大块连续KV缓存会造成未用部分浪费，vLLM的细颗粒度页分配和动态重用显著降低了内存开销。vLLM 在高并发、长上下文场景下对比 HuggingFace Transformers 或 FasterTransformer 等框架**吞吐提升可达2-4倍**。

vLLM 针对自回归生成的特点，**实现了连续异步批处理**：不再等待整个批次所有请求完成后再处理下一个批次，而是**实时将新请求插入**正在进行的解码循环中。这种**动态批调度**让已经完成生成的请求腾出计算槽位，新的请求立即填补，从而**保持GPU满负荷工作**，减少等待和空闲时间。

在调度算法上，vLLM 可以配置**Auto-Batching**策略，根据当前每个序列的进度（如何时需要下一轮前向计算）来决定批次组成，实现真正的流水线并行。为降低CUDA启动开销，vLLM 还运用了 **CUDA Graph** 技术：在初始化时捕获代表不同batch大小的计算图，后续推理直接复用预编译图，从而减少kernel launch overhead，进一步压低了单token生成的延迟。



Model Executor 模型执行器：这个组件负责实际的模型前向计算。vLLM 为每张GPU 创建一个Worker线程/进程运行模型。在单机多卡时，默认采用**UniProcExecutor**(单进程，多线程模式)来驱动单GPU计算，但也提供 **MultiProcExecutor** 支持多进程多GPU并行。初始化时，引擎会为每个Worker**绑定对应的CUDA设备**并检查显存。随后加载模型权重到显存并调用 model.eval开启推理模式。如果配置启用，会对模型执行 torch.compile() 或 CUDA Graph 预编译，加速后续推理。vLLM 加载模型时会**预分配一定比例的 GPU 内存**用于 KV 缓存（默认 80%)。



Input/Output 处理：在调度和模型执行之外，vLLM 还包含 Processor(输入处理器)和 Output Processor 模块，他们负责它们负责将用户文本请求转为 token 序列，包装成内部请求对象，并在模型生成后将 token 序列还原为文本输出。vLLM 支持多种采样策略（如温度、top-k等）用于生成下一个 token。这些输入/输出处理与核心推理解耦，以保持架构清晰。

