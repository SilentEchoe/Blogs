---
title: Kubernetes 原生的高性能分布式 LLM 推理框架
date: 2025-12-25 10:542:00
tags: [人工智能,Agent]
category: AI
---

大规模语言模型(LLM)在线推理需要解决高吞吐，低延迟以及高效的内存/显存利用等挑战。这进一步催生了云原生在AI基础设施的需求，于是一系列基于Kubernetes 原生的高性能分布式LLM推理框架应运而生，这其中包括 **vLLM**、**TensorRT-LLM**、**FastServe**、**LightLLM** 等。它们通过创新的内存管理和调度策略，在Kubernetes环境下实现了高效的推理性能，优化的GPU资源调度，通过多副本的负载均衡以及模型热加载与弹性扩展等功能成为模型推理的基石。



### vLLM 高吞吐的连续批处理与分页注意力

vLLM 源于 UC Berkeley的开源项目，它本身是一个专为高吞吐，低延迟LLM推理设计的系统。它的创新点在于 分页的KV缓存管理 以及 连续批处理(Continuous Batching) 机制，通过这两个技术在高并发的场景下优于传统架构。
vLLM 主要围绕 **LLMEngine**展开，这是处理推理请求的核心类，它的内部包含模型执行，调度和缓存管理等子组件。vLLM V1 引擎由**引擎核心 (Engine Core)** 和**服务层 (Serving layer)**构成。引擎核心负责单机多GPU的高效推理，服务层则处理多实例分布式部署和异步 Web 服务封装。



Scheduler 集中调度器：这个调度器位于引擎核心位置，负责决策每一步迭代中哪些请求参与计算。同时因vLLM 调度器实现了连续批处理策略，每轮迭代动态加入新请求或等待的请求，这使GPU时刻满载。调度器内部维护两个队列：waiting 和 running。支持先到先服务或优先级调度模式。它还集成了 KV缓存管理器，负责分配和释放 KV缓存的内存块，是分页注意力的核心。
KV 缓存管理：vLLM采用 **PagedAttention**算法，将注意力的 KV 缓存拆分为固定大小的"页"(内存块)管理。每个请求不再预先占用一整块连续显存，而是在需要时动态分配页面。一个轻量级的页表维护逻辑序列到无力内存块的映射，实现按需加载和服用。多个请求若有相同前缀，可共享相应的 KV 缓存页（写时复制机制），极大提高内存利用率。

这很像Linux中的分页，这种按序列整页交换避免了碎片和频繁拷贝的开销，通过实测 PagedAttention 可将 KV 缓存利用率提升到 90%以上，而传统连续内存分配往往只有 20~40% 利用率。如果显存不足，vLLM甚至会将整个请求的 KV 数据分页换出到 CPU 内存，在需要时换入。



Model Executor 模型执行器：这个组件负责实际的模型前向计算。vLLM 为每张GPU 创建一个Worker线程/进程运行模型。在单机多卡时，默认采用**UniProcExecutor**(单进程，多线程模式)来驱动单GPU计算，但也提供 **MultiProcExecutor** 支持多进程多GPU并行。初始化时，引擎会为每个Worker**绑定对应的CUDA设备**并检查显存。随后加载模型权重到显存并调用 model.eval

