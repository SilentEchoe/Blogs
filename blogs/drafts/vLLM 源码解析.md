---
title: vLLM 源码浅析
date: 2026-01-31 10:542:00
tags: [人工智能,Agent]
category: AI
---

随着大语言模型能力不断衍进，以LLM为基座的服务爆发性增长，这对模型的推理能力提出了新的挑战，不可避免需要同时满足**高吞吐、低延迟**，并实现**内存/显存的高效利用**等关键目标。这无疑推动了云原生在AI基础设施中的落地，一系列围绕着 Kubernetes 原生构建的高性能分布式推理框架应运而生。这些框架通过更先进的**显存管理**与**请求调度**策略，在 Kubernetes 环境中实现接近硬件上限的推理效率，并进一步提供**GPU 资源调度优化**、**多副本负载均衡**、**模型热加载**与**弹性伸缩**等能力，逐步成为现代模型推理服务的基础设施底座。

vLLM 源于UC Berkeley的开源项目，它本身是一个专为高吞吐，低延迟LLM推理设计的系统。它的核心要点在于**分页的KV缓存管理**和**连续批处理机制(Continuous Batching)**，本篇文章主要围绕vLLM处理推理请求的核心展开，会逐一解析其中的关键部分。





### Scheduler 调度器

Scheduler 负责维护请求队列，它还可以控制每个 Engine 步骤处理那些请求，以及每个请求处理多少Token。vLLM的调度器需要实时感知GPU KV缓存占用情况以及每个请求的状态。

在**Prefill阶段**(需要解释)可以按批处理尽可能多的prompt token,**decode阶段**按序逐步生成。







### PagedAttention KV缓存

vLLM 采用**PagedAttention**算法将注意力的 **KV缓存** 拆分为固定大小的“页”(内存块)进行管理，请求不再预先占用一整块连续显存，而是在需要的时候做动态分配页。一个轻量级的页表维护逻辑序列到物理内存块的映射，直接实现按需加载和复用。若多个请求带有相同的前缀则可共享对应的KV缓存页。

这种方式无疑是参照了Linux中的Cache Page,按序列整页交换避免了碎片和频繁拷贝的开销，传统连续内存分配往往只有20%～40%的利用率，PagedAttention 可将 KV 缓存利用率提升到 90%以上。

当显存不足时，vLLM甚至会将整个请求的KV数据分页换出道CPU内存，在需要的时候再进行换出，通过这种方式极大提高了内存利用率。



