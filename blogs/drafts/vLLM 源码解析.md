---
title: vLLM 源码浅析
date: 2026-01-31 10:542:00
tags: [人工智能,Agent]
category: AI
---

随着大语言模型能力不断衍进，以LLM为基座的服务爆发性增长，这对推理能力提出了新的挑战，推理模型需要同时满足**高吞吐、低延迟**，并实现**内存/显存的高效利用**等关键目标。这进一步推动了云原生在AI基础设施中的落地，一系列围绕着 Kubernetes 原生构建的高性能分布式推理框架应运而生。这些框架通过更先进的**显存管理**与**请求调度**策略，在 Kubernetes 环境中实现接近硬件上限的推理效率，并进一步提供**GPU 资源调度优化**、**多副本负载均衡**、**模型热加载**与**弹性伸缩**等能力，逐步成为现代模型推理服务的基础设施底座。



vLLM 源于UC Berkeley的开源项目，它本身是一个专为高吞吐，低延迟LLM推理设计的系统。它的核心要点在于**分页的KV缓存管理**和**连续批处理机制(Continuous Batching)**，本篇文章主要围绕vLLM处理推理请求的核心展开，会逐一解析其中的关键部分。



### PagedAttention KV缓存

vLLM 采用**PagedAttention**算法将注意力的 **KV缓存** 拆分为固定大小的“页”(内存块)管理，

